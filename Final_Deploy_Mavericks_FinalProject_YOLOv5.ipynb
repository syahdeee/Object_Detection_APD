{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo8Ub86MEniR"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUMG3-vFEq05",
        "outputId": "49328bc2-68e5-4edc-e6e6-930ed574323f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 9507, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 9507 (delta 0), reused 2 (delta 0), pack-reused 9504\u001b[K\n",
            "Receiving objects: 100% (9507/9507), 11.74 MiB | 17.73 MiB/s, done.\n",
            "Resolving deltas: 100% (6354/6354), done.\n",
            "/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "# clone Rickyfazaa - YOLOv5 repository\n",
        "!git clone https://github.com/rickyfazaa/yolov5.git  # clone repo\n",
        "# masuk kedalam YOLOv5 directories (folder)\n",
        "%cd yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7sfakMsS2Ii"
      },
      "source": [
        "# Ganti Detect.py yang sudah diedit oleh tim Mavericks (conf diedit Manual ke 0.45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95cN5RPBS5_R",
        "outputId": "e20e5810-ebca-40d7-bf70-a9e105c34c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F-usvWkf_cX8u5dJvy46LAGZlOUsfT-w\n",
            "To: /content/yolov5/detect.py\n",
            "\r  0% 0.00/13.6k [00:00<?, ?B/s]\r100% 13.6k/13.6k [00:00<00:00, 56.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# detect.py link : https://drive.google.com/file/d/1F-usvWkf_cX8u5dJvy46LAGZlOUsfT-w/view?usp=sharing\n",
        "!gdown https://drive.google.com/uc?id=1F-usvWkf_cX8u5dJvy46LAGZlOUsfT-w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na7x-NJ2EtEp",
        "outputId": "4bb10cf5-85ca-4650-ee37-748edf376bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetup complete. Using torch 2.0.1+cu118 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies as necessary\n",
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
        "import torch\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.downloads import attempt_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3s4zWWUeJFC"
      },
      "source": [
        "# Model Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKGMeEHLdWnD",
        "outputId": "a05fc364-ffbb-43ed-e2ea-540120bd7f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YUZ_dQMIB9-qn6IMiBNLckbqijkl2KjR\n",
            "To: /content/yolov5/best.pt\n",
            "\r  0% 0.00/14.8M [00:00<?, ?B/s]\r 60% 8.91M/14.8M [00:00<00:00, 74.5MB/s]\r100% 14.8M/14.8M [00:00<00:00, 108MB/s] \n"
          ]
        }
      ],
      "source": [
        "# My https://drive.google.com/file/d/1F1tx_n2IW-b8SO93DgGbm3_4l6wQ6hvU/view?usp=share_link\n",
        "# Arsi 1 https://drive.google.com/file/d/18DgugODjbbXNh7BRWOYGjlCsutJoxc2y/view?usp=share_link\n",
        "# 300 https://drive.google.com/file/d/1YUZ_dQMIB9-qn6IMiBNLckbqijkl2KjR/view?usp=share_link\n",
        "!gdown https://drive.google.com/uc?id=1YUZ_dQMIB9-qn6IMiBNLckbqijkl2KjR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ3QeqiZeElJ"
      },
      "source": [
        "# Membuat folder untuk input gambar dan output gambar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0NIea35Tkxi"
      },
      "source": [
        "## **[Opsional]** untuk menghapus directory kalo gambar udah kepenuhan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-AA9ECPQicD"
      },
      "outputs": [],
      "source": [
        "!rm -rf input\n",
        "!rm -rf output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZV9i2V7Tq-C"
      },
      "source": [
        "## Make Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7c17xuPd6JE"
      },
      "outputs": [],
      "source": [
        "!mkdir input\n",
        "!mkdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkbmL4zNeO5X"
      },
      "source": [
        "# Requirements untuk Yolov5 Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXi1ie5ReD3J",
        "outputId": "04ded9a9-6cca-49b2-ad60-2610d4ff2d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing req.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile req.txt\n",
        "# YOLOv5 requirements\n",
        "# Usage: pip install -r requirements.txt\n",
        "\n",
        "# Base ----------------------------------------\n",
        "matplotlib>=3.2.2\n",
        "numpy>=1.18.5\n",
        "opencv-python\n",
        "Pillow>=7.1.2\n",
        "PyYAML>=5.3.1\n",
        "requests>=2.23.0\n",
        "scipy>=1.4.1  # Google Colab version\n",
        "torch>=1.7.0\n",
        "torchvision>=0.8.1\n",
        "tqdm>=4.41.0\n",
        "protobuf<4.21.5  # https://github.com/ultralytics/yolov5/issues/8012\n",
        "ultralytics\n",
        "# Logging -------------------------------------\n",
        "tensorboard>=2.4.1\n",
        "# wandb\n",
        "\n",
        "# Plotting ------------------------------------\n",
        "pandas>=1.1.4\n",
        "seaborn>=0.11.0\n",
        "\n",
        "\n",
        "# Extras --------------------------------------\n",
        "ipython  # interactive notebook\n",
        "psutil  # system utilization\n",
        "thop  # FLOPs computation\n",
        "streamlit\n",
        "wget\n",
        "ffmpeg-python\n",
        "pyngrok\n",
        "streamlit_webrtc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMC1WGoAeTsO",
        "outputId": "73b9a895-a39f-4ba8-ee7e-586f302f249e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 6)) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 7)) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 8)) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 10)) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 12)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 13)) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 14)) (4.65.0)\n",
            "Requirement already satisfied: protobuf<4.21.5 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 15)) (3.20.3)\n",
            "Collecting ultralytics (from -r req.txt (line 16))\n",
            "  Downloading ultralytics-8.0.112-py3-none-any.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.1/593.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 18)) (2.12.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 22)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 23)) (0.12.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 27)) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 28)) (5.9.5)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 29)) (0.1.1.post2209072238)\n",
            "Collecting streamlit (from -r req.txt (line 30))\n",
            "  Downloading streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget (from -r req.txt (line 31))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python (from -r req.txt (line 32))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting pyngrok (from -r req.txt (line 33))\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting streamlit_webrtc (from -r req.txt (line 34))\n",
            "  Downloading streamlit_webrtc-0.45.0-py3-none-any.whl (872 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.3/872.3 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r req.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r req.txt (line 10)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r req.txt (line 10)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r req.txt (line 10)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r req.txt (line 10)) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r req.txt (line 12)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->-r req.txt (line 12)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->-r req.txt (line 12)) (16.0.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r req.txt (line 18)) (0.40.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r req.txt (line 22)) (2022.7.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r req.txt (line 27)) (4.8.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (4.2.2)\n",
            "Collecting blinker<2,>=1.0.0 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (5.3.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (8.1.3)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (13.3.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (8.2.2)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (4.3)\n",
            "Collecting validators<1,>=0.2 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit->-r req.txt (line 30))\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r req.txt (line 30)) (6.3.1)\n",
            "Collecting watchdog (from streamlit->-r req.txt (line 30))\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->-r req.txt (line 32)) (0.18.3)\n",
            "Collecting aiortc<2.0.0,>=1.4.0 (from streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading aiortc-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aioice<1.0.0,>=0.9.0 (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading aioice-0.9.0-py3-none-any.whl (24 kB)\n",
            "Collecting av<11.0.0,>=9.0.0 (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34)) (1.15.1)\n",
            "Requirement already satisfied: cryptography>=2.2 in /usr/local/lib/python3.10/dist-packages (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34)) (40.0.2)\n",
            "Requirement already satisfied: google-crc32c>=1.1 in /usr/local/lib/python3.10/dist-packages (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34)) (1.5.0)\n",
            "Collecting pyee>=9.0.0 (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading pyee-9.1.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pylibsrtp>=0.5.6 (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading pylibsrtp-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyopenssl>=23.1.0 (from aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r req.txt (line 30)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r req.txt (line 30)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r req.txt (line 30)) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit->-r req.txt (line 30))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r req.txt (line 18)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r req.txt (line 18)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r req.txt (line 18)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r req.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->-r req.txt (line 30)) (3.15.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r req.txt (line 27)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r req.txt (line 27)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r req.txt (line 27)) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->-r req.txt (line 12)) (2.1.2)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit->-r req.txt (line 30)) (2.2.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal<5,>=1.1->streamlit->-r req.txt (line 30)) (0.1.0.post0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->-r req.txt (line 12)) (1.3.0)\n",
            "Collecting dnspython>=2.0.0 (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ifaddr>=0.2.0 (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34))\n",
            "  Downloading ifaddr-0.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->aiortc<2.0.0,>=1.4.0->streamlit_webrtc->-r req.txt (line 34)) (2.21)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit->-r req.txt (line 30))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r req.txt (line 30)) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r req.txt (line 30)) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.11.0->streamlit->-r req.txt (line 30)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r req.txt (line 18)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r req.txt (line 18)) (3.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->-r req.txt (line 30)) (2023.3)\n",
            "Building wheels for collected packages: wget, pyngrok, validators\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=0be4b184a62f7f6e4abd11a623511b522d605891e68f8e6749fe43a4823c1ded\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=cb4161771a5ca8a99bd7170256498bba0068450ebdf0d2cf059786d8cf3ebe78\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=7395b3d53b9268290113ad3db58d6a93c1fa2482091a211aa157f06fbe6bb18b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
            "Successfully built wget pyngrok validators\n",
            "Installing collected packages: wget, ifaddr, av, watchdog, validators, smmap, pyngrok, pympler, pyee, importlib-metadata, ffmpeg-python, dnspython, blinker, pylibsrtp, pydeck, gitdb, aioice, pyopenssl, gitpython, streamlit, aiortc, streamlit_webrtc, ultralytics\n",
            "Successfully installed aioice-0.9.0 aiortc-1.5.0 av-10.0.0 blinker-1.6.2 dnspython-2.3.0 ffmpeg-python-0.2.0 gitdb-4.0.10 gitpython-3.1.31 ifaddr-0.2.0 importlib-metadata-6.6.0 pydeck-0.8.1b0 pyee-9.1.0 pylibsrtp-0.8.0 pympler-1.0.1 pyngrok-6.0.0 pyopenssl-23.2.0 smmap-5.0.0 streamlit-1.23.1 streamlit_webrtc-0.45.0 ultralytics-8.0.112 validators-0.20.0 watchdog-3.0.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r req.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufqhmIQriQCQ"
      },
      "source": [
        "# Write python file for streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGwpQ7f6ei5Y",
        "outputId": "cd5d4284-4e36-4dc0-d7b8-f7813108663b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing video_predict.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile video_predict.py\n",
        "import os\n",
        "import os.path as osp\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "def runVideo(model, video, vdo_view, warn):\n",
        "    video_name = osp.basename(video)\n",
        "    outputpath = osp.join('output', video_name)\n",
        "\n",
        "    # Create A Dir to save Video Frames\n",
        "    frames_dir = osp.join('input')\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    frame_count = 0\n",
        "    with st.spinner(text=\"Predicting...\"):\n",
        "        warn.warning(\n",
        "            'This is realtime prediction, If you wish to download the final prediction result wait until the process done.', icon=\"⚠️\")\n",
        "        while True:\n",
        "            frame_count += 1\n",
        "            ret, frame = cap.read()\n",
        "            if ret == False:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            result = model(frame)\n",
        "            result.render()\n",
        "            image = Image.fromarray(result.imgs[0])\n",
        "            vdo_view.image(image, caption='Current Model Prediction(s)')\n",
        "            image.save(osp.join(frames_dir, f'{frame_count}.jpg'))\n",
        "        cap.release()\n",
        "        # convert frames in dir to a single video file\n",
        "        os.system(\n",
        "            f'ffmpeg -framerate 30 -i {frames_dir}/%d.jpg -c:v libx264 -pix_fmt yuv420p {outputpath}')\n",
        "    # Clean up Frames Dir\n",
        "    # os.system(f'rm -rf {frames_dir}')\n",
        "\n",
        "    # Display Video\n",
        "    output_video = open(outputpath, 'rb')\n",
        "    output_video_bytes = output_video.read()\n",
        "    st.video(output_video_bytes)\n",
        "    st.write(\"Model Prediction\")\n",
        "    vdo_view.empty()\n",
        "    warn.empty()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj1u2hSuFh-C"
      },
      "source": [
        "# Write main app.py for running streamlit\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h8hJCclFjwJ",
        "outputId": "39ca55ac-7280-41f5-a67d-c795c8d18668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "from io import *\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import os\n",
        "import wget\n",
        "import cv2 # Lupa di-import\n",
        "from streamlit import session_state\n",
        "from video_predict import runVideo\n",
        "# from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration, VideoProcessorBase, WebRtcMode\n",
        "# st.set_page_config(layout=\"wide\")\n",
        "import subprocess\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration\n",
        "import av\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"PPE Detection using Streamlit\",\n",
        "    page_icon=\"🛠️\",\n",
        ")\n",
        "\n",
        "latar = \"\"\"\n",
        "<style>\n",
        "[class=\"main css-uf99v8 e1g8pov65\"]{\n",
        "  background-image: url(\"https://telegra.ph/file/b99e84fd8c75391a1ebc9.png\");\n",
        "  background-repeat: no-repeat;\n",
        "  background-size: cover;\n",
        "}\n",
        "\n",
        "\n",
        "[class=\"css-vk3wp9 e1akgbir11\"]{\n",
        "  background-image: url(\"https://telegra.ph/file/1aa88f365322b1972bf3f.png\");\n",
        "  background-repeat: no-repeat;\n",
        "  background-size: cover;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "[data-testid=\"stHeader\"]{\n",
        "  background-image: url(\"https://telegra.ph/file/9d4c3a7cbd1b60350d814.png\");\n",
        "  background-repeat: no-repeat;\n",
        "  background-size: cover;\n",
        "}\n",
        "\n",
        "[class=\"css-1fcdlhc e1mp27150\"]{\n",
        "  background-image: url(\"https://telegra.ph/file/33b405b81b59f0cfec2d0.png\");\n",
        "  background-repeat: no-repeat;\n",
        "  background-size: cover;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "st.markdown(latar, unsafe_allow_html=True)\n",
        "\n",
        "# Configurations\n",
        "CFG_MODEL_PATH = \"best.pt\"\n",
        "CFG_ENABLE_URL_DOWNLOAD = False\n",
        "CFG_ENABLE_VIDEO_PREDICTION = True\n",
        "if CFG_ENABLE_URL_DOWNLOAD:\n",
        "    # Configure this if you set cfg_enable_url_download to True\n",
        "    url = \"https://drive.google.com/uc?id=1OAU7ruLtsiMQE1krPRIRhYwcEkQdMLfD\"\n",
        "# End of Configurations\n",
        "\n",
        "def imageInput(model, src):\n",
        "\n",
        "    if src == '📁 Upload your own data.':\n",
        "        image_file = st.file_uploader(\n",
        "            \":open_file_folder: Upload An 📸 (Image)\", type=['png', 'jpeg', 'jpg'])\n",
        "        col1, col2 = st.columns(2)\n",
        "        if image_file is not None:\n",
        "            img = Image.open(image_file)\n",
        "            with col1:\n",
        "                st.image(img, caption='Uploaded Image',\n",
        "                         use_column_width='always') # use_column_width='always' width=256\n",
        "            ts = datetime.timestamp(datetime.now())\n",
        "            imgpath = os.path.join('input', str(ts)+image_file.name)\n",
        "            # outputpath = os.path.join('outputaw', os.path.basename(imgpath))\n",
        "            with open(imgpath, mode=\"wb\") as f:\n",
        "                f.write(image_file.getbuffer())\n",
        "\n",
        "            with st.spinner(text=\"Predicting...\"):\n",
        "                # Load model\n",
        "                # cap = cv2.VideoCapture(imgpath)\n",
        "                # ret, frame = cap.read()\n",
        "                #frame = cv2.imread(imgpath)\n",
        "                #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                #pred = model(frame)\n",
        "                #pred.render()\n",
        "                # save output to file\n",
        "                #for im in pred.imgs:\n",
        "                    #im_base64 = Image.fromarray(im)\n",
        "                    #im_base64.save(outputpath)\n",
        "                # subprocess.run(\"ls\")\n",
        "                subprocess.run(['python3', 'detect.py', '--weights', CFG_MODEL_PATH, '--img', '256', '--conf', '0.4', '--source', imgpath])\n",
        "\n",
        "            # Predictions\n",
        "            output_imgpath = os.path.join('output',str(ts)+image_file.name)\n",
        "            img_ = Image.open(output_imgpath)\n",
        "            with col2:\n",
        "                st.image(img_, caption='Model Prediction(s)',\n",
        "                         use_column_width='always') # use_column_width='always'\n",
        "\n",
        "            if \"button_clicked\" not in st.session_state:\n",
        "                st.session_state.button_clicked = True\n",
        "\n",
        "            ini_slide_conf1 = str(0.4)\n",
        "            ini_slide_conf2 = st.slider(\"Slider of Confidence Threshold between 0.0 - 0.8\", value=0.4, min_value=0.0, max_value=0.8, step=0.01)\n",
        "            if ini_slide_conf1 != str(ini_slide_conf2):\n",
        "                ini_slide_conf1 = str(ini_slide_conf2)\n",
        "                with st.spinner(text=\"Load Result...\"):\n",
        "                    subprocess.run(['python3', 'detect.py', '--weights', CFG_MODEL_PATH, '--img', '256', '--conf', str(ini_slide_conf2), '--source', imgpath])\n",
        "                    output_imgpath = os.path.join('output', str(ts)+image_file.name)\n",
        "                    img_ = Image.open(output_imgpath)\n",
        "                    st.image(img_, caption=f'Model Prediction(s) : Confidence Threshold = {ini_slide_conf1}',use_column_width='always') # use_column_width='always'\n",
        "                col1, col2, col3 , col4, col5 = st.columns(5)\n",
        "                with col1:\n",
        "                    pass\n",
        "                with col2:\n",
        "                    pass\n",
        "                with col4:\n",
        "                    pass\n",
        "                with col5:\n",
        "                    pass\n",
        "                with col3 :\n",
        "                    with open(output_imgpath, \"rb\") as file:\n",
        "                        btn = st.download_button(\n",
        "                                label=\"Download image\",\n",
        "                                data=file,\n",
        "                                file_name=str(ts)+image_file.name,\\\n",
        "                              )\n",
        "\n",
        "\n",
        "def videoInput(model, src):\n",
        "    if src == '📁 Upload your own data.':\n",
        "        uploaded_video = st.file_uploader(\n",
        "            \":open_file_folder: Upload A 📽️ (Video)\", type=['mp4', 'mpeg', 'mov'])\n",
        "        pred_view = st.empty()\n",
        "        warning = st.empty()\n",
        "        if uploaded_video != None:\n",
        "\n",
        "            # Save video to disk\n",
        "            ts = datetime.timestamp(datetime.now())  # timestamp a upload\n",
        "            uploaded_video_path = os.path.join(\n",
        "                'input', str(ts)+uploaded_video.name)\n",
        "            with open(uploaded_video_path, mode='wb') as f:\n",
        "                f.write(uploaded_video.read())\n",
        "\n",
        "            # Display uploaded video\n",
        "            with open(uploaded_video_path, 'rb') as f:\n",
        "                video_bytes = f.read()\n",
        "            st.video(video_bytes)\n",
        "            st.write(\"Uploaded Video\")\n",
        "            submit = st.button(\"Run Prediction\")\n",
        "            if submit:\n",
        "                runVideo(model, uploaded_video_path, pred_view, warning)\n",
        "\n",
        "    elif src == 'From example data.':\n",
        "        # Image selector slider\n",
        "        videopaths = glob.glob('data/example_videos/*')\n",
        "        if len(videopaths) == 0:\n",
        "            st.error(\n",
        "                'No videos found, Please upload example videos in data/example_videos', icon=\"⚠️\")\n",
        "            return\n",
        "        imgsel = st.slider('Select random video from example data.',\n",
        "                           min_value=1, max_value=len(videopaths), step=1)\n",
        "        pred_view = st.empty()\n",
        "        video = videopaths[imgsel-1]\n",
        "        submit = st.button(\"Predict!\")\n",
        "        if submit:\n",
        "            runVideo(model, video, pred_view, warning)\n",
        "\n",
        "def webcam_streamlit():\n",
        "        device = \"cpu\"\n",
        "        st.model = torch.hub.load('rickyfazaa/yolov5', 'custom',\n",
        "                           path=CFG_MODEL_PATH, force_reload=True, device=device, skip_validation=True)\n",
        "        RTC_CONFIGURATION = RTCConfiguration({\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]})\n",
        "\n",
        "\n",
        "        class VideoProcessor:\n",
        "            def recv(self, frame):\n",
        "                img = frame.to_ndarray(format=\"rgb24\")\n",
        "\n",
        "                # vision processing\n",
        "                flipped = img[:, ::-1, :]\n",
        "\n",
        "                # model processing\n",
        "                im_pil = Image.fromarray(flipped)\n",
        "                results = st.model(im_pil, size=112) #112\n",
        "                bbox_img = np.array(results.render()[0])\n",
        "\n",
        "                return av.VideoFrame.from_ndarray(bbox_img, format=\"rgb24\")\n",
        "\n",
        "\n",
        "        webrtc_ctx = webrtc_streamer(\n",
        "            key=\"WYH\",\n",
        "            mode=WebRtcMode.SENDRECV,\n",
        "            rtc_configuration=RTC_CONFIGURATION,\n",
        "            video_processor_factory=VideoProcessor,\n",
        "            media_stream_constraints={\"video\": True, \"audio\": False},\n",
        "            async_processing=False,\n",
        "        )\n",
        "\n",
        "\n",
        "def main():\n",
        "    # -- Sidebar\n",
        "    st.sidebar.title(\"\"\"                 ![image](https://telegra.ph/file/5565c294e12e43977b1af.png)\"\"\") # 100px\n",
        "    st.sidebar.write(\" \")\n",
        "\n",
        "    st.sidebar.header('''Hello! Welcome 🖐️''')\n",
        "    st.sidebar.subheader(\"⚙️ Options\")\n",
        "\n",
        "    if CFG_ENABLE_VIDEO_PREDICTION:\n",
        "        option = st.sidebar.selectbox(\"🎯 Select Activity.\", ['🌟 Home', '📸 Deteksi Melalui Image', '📽️ Deteksi Melalui Video', '📹 Real-time'])\n",
        "    else:\n",
        "        option = st.sidebar.radio(\"Select Activity.\", ['Home','Image'])\n",
        "\n",
        "    datasrc = st.sidebar.radio(\"🌀 Input source.\", [\n",
        "                               '📁 Upload your own data.'],disabled=False)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        deviceoption = st.sidebar.radio(\"Select compute Device.\", [\n",
        "                                        'cpu'], disabled=True, index=0)\n",
        "\n",
        "    else:\n",
        "        deviceoption = st.sidebar.radio(\"Select compute Device.\", [\n",
        "                                        'cpu', 'cuda'], disabled=True, index=0)\n",
        "    # -- End of Sidebar\n",
        "\n",
        "    st.header('🖥️ Penerapan Teknologi Object Detection dalam Identifikasi Pemakaian APD pada Pekerja Konstruksi 🔨')\n",
        "    st.sidebar.write(\" \")\n",
        "    st.sidebar.write(\" \")\n",
        "    st.sidebar.write(\" \")\n",
        "    st.sidebar.write(\" \")\n",
        "    st.sidebar.markdown(\n",
        "        \"\"\"![image](https://telegra.ph/file/445ce9e4b20d64ed84f02.png)\n",
        "\n",
        "Developed by Mavericks\n",
        "Email : mvrteam11@gmail.com\"\"\")\n",
        "    st.sidebar.markdown(\"\"\"Contact : +6281381158080\n",
        "[Leader LinkedIn](https://id.linkedin.com/in/rickyfazaa)\n",
        "[Leader GitHub](https://github.com/rickyfazaa)\"\"\")\n",
        "\n",
        "    if option == \"📸 Deteksi Melalui Image\":\n",
        "        imageInput(loadmodel(deviceoption), datasrc)\n",
        "    elif option == \"📽️ Deteksi Melalui Video\":\n",
        "        videoInput(loadmodel(deviceoption), datasrc)\n",
        "    elif option == \"📹 Real-time\":\n",
        "        webcam_streamlit()\n",
        "    elif option == '🌟 Home':\n",
        "        html_temp_home1 = \"\"\"<div style=\"background-color:#6D7B8D;padding:10px\">\n",
        "                                            <h4 style=\"color:white;text-align:center;\">\n",
        "                                            Helm, Vest, dan Person detection menggunakan YOLOv5 Custom Model dan Streamlit</h4>\n",
        "                                            </div>\n",
        "                                            </br>\"\"\"\n",
        "        st.markdown(html_temp_home1, unsafe_allow_html=True)\n",
        "        st.write(\"\"\"\n",
        "                 Aplikasi ini memiliki tiga fungsi, yaitu\n",
        "\n",
        "                 1. Fungsi untuk mendeteksi APD (Alat Pelindung Diri) pekerja konstruksi pada Gambar.\n",
        "\n",
        "                 2. Fungsi untuk mendeteksi APD (Alat Pelindung Diri) pekerja konstruksi pada Video.\n",
        "\n",
        "                 3. Fungsi untuk mendeteksi APD (Alat Pelindung Diri) pekerja konstruksi secara REAL-TIME Webcam.\n",
        "\n",
        "                 \"\"\")\n",
        "        st.write(\" \")\n",
        "        st.write(\" \")\n",
        "        st.write(\" \")\n",
        "        with st.expander(\"🚀 About Team Profile\"):\n",
        "            st.write(\"# Mavericks Team - Startup Campus\")\n",
        "            col1, col2, col3 , col4, col5 = st.columns(5)\n",
        "            with col1:\n",
        "                pass\n",
        "            with col2:\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.image(\"https://telegra.ph/file/ecce29e0e57aebb4a2c20.png\", caption = \"@Kampus Merdeka Logo\")\n",
        "            with col4:\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.write(\" \")\n",
        "                st.image(\"https://telegra.ph/file/1d478d3c83fa221ffdbd4.png\", caption = \"@Startup Campus Logo\")\n",
        "            with col5:\n",
        "                pass\n",
        "            with col3 :\n",
        "                st.image(\"https://telegra.ph/file/445ce9e4b20d64ed84f02.png\", caption = \"@Mavericks Logo\")\n",
        "            st.write(\" \")\n",
        "            st.write(\"**Supervisor**  - M. Haswin Anugrah Pratama\")\n",
        "            st.write(\"**Facilitator** - Siska Hamelia Putri\")\n",
        "            st.write(\"**Facilitator** - Ni Luh Nitya Ayu Laksmi\")\n",
        "            st.write(\" \")\n",
        "            st.write(\"**_Team_**\")\n",
        "            st.write(\"**Ricky Khairul Faza (Leader)** - AI04023 - Institut Teknologi Indonesia\")\n",
        "            st.write(\"**Nurul Qorimah Reski**         - AI04019 - Institut Teknologi Perusahaan Listrik Negara\")\n",
        "            st.write(\"**Aisyah Dliya Ramadhanti**     - AI04028 - Universitas Telkom\")\n",
        "            st.write(\"**Lisa Yiha Rodhiatun**         - AI04038 - STMIK Bandung\")\n",
        "            st.write(\"**Isrul Pati Sianturi**         - AI04060 - Universitas Medan Area\")\n",
        "            st.write(\"**Luthfiya Rifqi Bahasuan**     - AI04062 - Institut Teknologi Indonesia\")\n",
        "            st.write(\" \")\n",
        "            st.write(\" \")\n",
        "            st.write(\" \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Downlaod Model from url.\n",
        "@st.cache_resource\n",
        "def downloadModel():\n",
        "    if not os.path.exists(CFG_MODEL_PATH):\n",
        "        wget.download(url, out=\"models/\")\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def loadmodel(device):\n",
        "    model = torch.hub.load('rickyfazaa/yolov5', 'custom',\n",
        "                           path=CFG_MODEL_PATH, force_reload=True, device=device)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg1klzQCiI5v"
      },
      "source": [
        "# Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv2xVQf8ggSF",
        "outputId": "0f45d903-716d-42a1-ebd9-2bed98603a13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "INFO:yolov5:YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "INFO:yolov5:Fusing layers... \n",
            "Fusing layers... \n",
            "INFO:yolov5:custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "INFO:yolov5:Adding AutoShape... \n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AutoShape(\n",
              "  (model): DetectMultiBackend(\n",
              "    (model): Model(\n",
              "      (model): Sequential(\n",
              "        (0): Focus(\n",
              "          (conv): Conv(\n",
              "            (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Conv(\n",
              "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (2): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): Conv(\n",
              "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (4): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): Conv(\n",
              "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (6): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (1): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "            (2): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): Conv(\n",
              "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (8): SPP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv(\n",
              "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (m): ModuleList(\n",
              "            (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
              "            (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
              "            (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (10): Conv(\n",
              "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
              "        (12): Concat()\n",
              "        (13): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (14): Conv(\n",
              "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
              "        (16): Concat()\n",
              "        (17): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (18): Conv(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (19): Concat()\n",
              "        (20): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (21): Conv(\n",
              "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (22): Concat()\n",
              "        (23): BottleneckCSP(\n",
              "          (cv1): Conv(\n",
              "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (cv2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (cv4): Conv(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (act): SiLU(inplace=True)\n",
              "          )\n",
              "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (act): SiLU(inplace=True)\n",
              "          (m): Sequential(\n",
              "            (0): Bottleneck(\n",
              "              (cv1): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "              (cv2): Conv(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "                (act): SiLU(inplace=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (24): Detect(\n",
              "          (m): ModuleList(\n",
              "            (0): Conv2d(128, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (2): Conv2d(512, 30, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "CFG_MODEL_PATH = \"/content/yolov5/best.pt\"\n",
        "device = \"cpu\"\n",
        "model = torch.hub.load('rickyfazaa/yolov5', 'custom',\n",
        "                           path=CFG_MODEL_PATH, force_reload=True, device=device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f4oUsi6hhdL",
        "outputId": "40935c4d-33a0-44f1-e653-a9f6553c3474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "model.model.model.0.conv.conv.weight \t torch.Size([32, 12, 3, 3])\n",
            "model.model.model.0.conv.conv.bias \t torch.Size([32])\n",
            "model.model.model.1.conv.weight \t torch.Size([64, 32, 3, 3])\n",
            "model.model.model.1.conv.bias \t torch.Size([64])\n",
            "model.model.model.2.cv1.conv.weight \t torch.Size([32, 64, 1, 1])\n",
            "model.model.model.2.cv1.conv.bias \t torch.Size([32])\n",
            "model.model.model.2.cv2.weight \t torch.Size([32, 64, 1, 1])\n",
            "model.model.model.2.cv3.weight \t torch.Size([32, 32, 1, 1])\n",
            "model.model.model.2.cv4.conv.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.2.cv4.conv.bias \t torch.Size([64])\n",
            "model.model.model.2.bn.weight \t torch.Size([64])\n",
            "model.model.model.2.bn.bias \t torch.Size([64])\n",
            "model.model.model.2.bn.running_mean \t torch.Size([64])\n",
            "model.model.model.2.bn.running_var \t torch.Size([64])\n",
            "model.model.model.2.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.2.m.0.cv1.conv.weight \t torch.Size([32, 32, 1, 1])\n",
            "model.model.model.2.m.0.cv1.conv.bias \t torch.Size([32])\n",
            "model.model.model.2.m.0.cv2.conv.weight \t torch.Size([32, 32, 3, 3])\n",
            "model.model.model.2.m.0.cv2.conv.bias \t torch.Size([32])\n",
            "model.model.model.3.conv.weight \t torch.Size([128, 64, 3, 3])\n",
            "model.model.model.3.conv.bias \t torch.Size([128])\n",
            "model.model.model.4.cv1.conv.weight \t torch.Size([64, 128, 1, 1])\n",
            "model.model.model.4.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.cv2.weight \t torch.Size([64, 128, 1, 1])\n",
            "model.model.model.4.cv3.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.4.cv4.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.4.cv4.conv.bias \t torch.Size([128])\n",
            "model.model.model.4.bn.weight \t torch.Size([128])\n",
            "model.model.model.4.bn.bias \t torch.Size([128])\n",
            "model.model.model.4.bn.running_mean \t torch.Size([128])\n",
            "model.model.model.4.bn.running_var \t torch.Size([128])\n",
            "model.model.model.4.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.4.m.0.cv1.conv.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.4.m.0.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.m.0.cv2.conv.weight \t torch.Size([64, 64, 3, 3])\n",
            "model.model.model.4.m.0.cv2.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.m.1.cv1.conv.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.4.m.1.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.m.1.cv2.conv.weight \t torch.Size([64, 64, 3, 3])\n",
            "model.model.model.4.m.1.cv2.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.m.2.cv1.conv.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.4.m.2.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.4.m.2.cv2.conv.weight \t torch.Size([64, 64, 3, 3])\n",
            "model.model.model.4.m.2.cv2.conv.bias \t torch.Size([64])\n",
            "model.model.model.5.conv.weight \t torch.Size([256, 128, 3, 3])\n",
            "model.model.model.5.conv.bias \t torch.Size([256])\n",
            "model.model.model.6.cv1.conv.weight \t torch.Size([128, 256, 1, 1])\n",
            "model.model.model.6.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.cv2.weight \t torch.Size([128, 256, 1, 1])\n",
            "model.model.model.6.cv3.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.6.cv4.conv.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.6.cv4.conv.bias \t torch.Size([256])\n",
            "model.model.model.6.bn.weight \t torch.Size([256])\n",
            "model.model.model.6.bn.bias \t torch.Size([256])\n",
            "model.model.model.6.bn.running_mean \t torch.Size([256])\n",
            "model.model.model.6.bn.running_var \t torch.Size([256])\n",
            "model.model.model.6.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.6.m.0.cv1.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.6.m.0.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.m.0.cv2.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.6.m.0.cv2.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.m.1.cv1.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.6.m.1.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.m.1.cv2.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.6.m.1.cv2.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.m.2.cv1.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.6.m.2.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.6.m.2.cv2.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.6.m.2.cv2.conv.bias \t torch.Size([128])\n",
            "model.model.model.7.conv.weight \t torch.Size([512, 256, 3, 3])\n",
            "model.model.model.7.conv.bias \t torch.Size([512])\n",
            "model.model.model.8.cv1.conv.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.8.cv1.conv.bias \t torch.Size([256])\n",
            "model.model.model.8.cv2.conv.weight \t torch.Size([512, 1024, 1, 1])\n",
            "model.model.model.8.cv2.conv.bias \t torch.Size([512])\n",
            "model.model.model.9.cv1.conv.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.9.cv1.conv.bias \t torch.Size([256])\n",
            "model.model.model.9.cv2.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.9.cv3.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.9.cv4.conv.weight \t torch.Size([512, 512, 1, 1])\n",
            "model.model.model.9.cv4.conv.bias \t torch.Size([512])\n",
            "model.model.model.9.bn.weight \t torch.Size([512])\n",
            "model.model.model.9.bn.bias \t torch.Size([512])\n",
            "model.model.model.9.bn.running_mean \t torch.Size([512])\n",
            "model.model.model.9.bn.running_var \t torch.Size([512])\n",
            "model.model.model.9.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.9.m.0.cv1.conv.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.9.m.0.cv1.conv.bias \t torch.Size([256])\n",
            "model.model.model.9.m.0.cv2.conv.weight \t torch.Size([256, 256, 3, 3])\n",
            "model.model.model.9.m.0.cv2.conv.bias \t torch.Size([256])\n",
            "model.model.model.10.conv.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.10.conv.bias \t torch.Size([256])\n",
            "model.model.model.13.cv1.conv.weight \t torch.Size([128, 512, 1, 1])\n",
            "model.model.model.13.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.13.cv2.weight \t torch.Size([128, 512, 1, 1])\n",
            "model.model.model.13.cv3.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.13.cv4.conv.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.13.cv4.conv.bias \t torch.Size([256])\n",
            "model.model.model.13.bn.weight \t torch.Size([256])\n",
            "model.model.model.13.bn.bias \t torch.Size([256])\n",
            "model.model.model.13.bn.running_mean \t torch.Size([256])\n",
            "model.model.model.13.bn.running_var \t torch.Size([256])\n",
            "model.model.model.13.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.13.m.0.cv1.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.13.m.0.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.13.m.0.cv2.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.13.m.0.cv2.conv.bias \t torch.Size([128])\n",
            "model.model.model.14.conv.weight \t torch.Size([128, 256, 1, 1])\n",
            "model.model.model.14.conv.bias \t torch.Size([128])\n",
            "model.model.model.17.cv1.conv.weight \t torch.Size([64, 256, 1, 1])\n",
            "model.model.model.17.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.17.cv2.weight \t torch.Size([64, 256, 1, 1])\n",
            "model.model.model.17.cv3.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.17.cv4.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.17.cv4.conv.bias \t torch.Size([128])\n",
            "model.model.model.17.bn.weight \t torch.Size([128])\n",
            "model.model.model.17.bn.bias \t torch.Size([128])\n",
            "model.model.model.17.bn.running_mean \t torch.Size([128])\n",
            "model.model.model.17.bn.running_var \t torch.Size([128])\n",
            "model.model.model.17.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.17.m.0.cv1.conv.weight \t torch.Size([64, 64, 1, 1])\n",
            "model.model.model.17.m.0.cv1.conv.bias \t torch.Size([64])\n",
            "model.model.model.17.m.0.cv2.conv.weight \t torch.Size([64, 64, 3, 3])\n",
            "model.model.model.17.m.0.cv2.conv.bias \t torch.Size([64])\n",
            "model.model.model.18.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.18.conv.bias \t torch.Size([128])\n",
            "model.model.model.20.cv1.conv.weight \t torch.Size([128, 256, 1, 1])\n",
            "model.model.model.20.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.20.cv2.weight \t torch.Size([128, 256, 1, 1])\n",
            "model.model.model.20.cv3.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.20.cv4.conv.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.20.cv4.conv.bias \t torch.Size([256])\n",
            "model.model.model.20.bn.weight \t torch.Size([256])\n",
            "model.model.model.20.bn.bias \t torch.Size([256])\n",
            "model.model.model.20.bn.running_mean \t torch.Size([256])\n",
            "model.model.model.20.bn.running_var \t torch.Size([256])\n",
            "model.model.model.20.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.20.m.0.cv1.conv.weight \t torch.Size([128, 128, 1, 1])\n",
            "model.model.model.20.m.0.cv1.conv.bias \t torch.Size([128])\n",
            "model.model.model.20.m.0.cv2.conv.weight \t torch.Size([128, 128, 3, 3])\n",
            "model.model.model.20.m.0.cv2.conv.bias \t torch.Size([128])\n",
            "model.model.model.21.conv.weight \t torch.Size([256, 256, 3, 3])\n",
            "model.model.model.21.conv.bias \t torch.Size([256])\n",
            "model.model.model.23.cv1.conv.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.23.cv1.conv.bias \t torch.Size([256])\n",
            "model.model.model.23.cv2.weight \t torch.Size([256, 512, 1, 1])\n",
            "model.model.model.23.cv3.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.23.cv4.conv.weight \t torch.Size([512, 512, 1, 1])\n",
            "model.model.model.23.cv4.conv.bias \t torch.Size([512])\n",
            "model.model.model.23.bn.weight \t torch.Size([512])\n",
            "model.model.model.23.bn.bias \t torch.Size([512])\n",
            "model.model.model.23.bn.running_mean \t torch.Size([512])\n",
            "model.model.model.23.bn.running_var \t torch.Size([512])\n",
            "model.model.model.23.bn.num_batches_tracked \t torch.Size([])\n",
            "model.model.model.23.m.0.cv1.conv.weight \t torch.Size([256, 256, 1, 1])\n",
            "model.model.model.23.m.0.cv1.conv.bias \t torch.Size([256])\n",
            "model.model.model.23.m.0.cv2.conv.weight \t torch.Size([256, 256, 3, 3])\n",
            "model.model.model.23.m.0.cv2.conv.bias \t torch.Size([256])\n",
            "model.model.model.24.anchors \t torch.Size([3, 3, 2])\n",
            "model.model.model.24.m.0.weight \t torch.Size([30, 128, 1, 1])\n",
            "model.model.model.24.m.0.bias \t torch.Size([30])\n",
            "model.model.model.24.m.1.weight \t torch.Size([30, 256, 1, 1])\n",
            "model.model.model.24.m.1.bias \t torch.Size([30])\n",
            "model.model.model.24.m.2.weight \t torch.Size([30, 512, 1, 1])\n",
            "model.model.model.24.m.2.bias \t torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eg6QtRkASpx"
      },
      "source": [
        "## Arsitektur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVnjvVIn_fva"
      },
      "outputs": [],
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSRw9xtrAD0a"
      },
      "outputs": [],
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: 5  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.25  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, vggLayer, [64, 2]],   # 0 P1/2\n",
        "   [-1, 1, vggLayer, [128, 2]],  # 1 P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 2, vggLayer, [256, 2]],  # 2 P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 2, vggLayer, [512, 2]],  # 3 P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 2, vggLayer, [512, 2]],  # 4 P5/32\n",
        "    [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df5HG5tQBFgF"
      },
      "source": [
        "## Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlygRaP29dES",
        "outputId": "54f89578-8bde-484a-e55a-65733d5d0dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:yolov5:\n",
            "                 from  n    params  module                                  arguments                     \n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "INFO:yolov5:  0                -1  1      1792  models.common.vggLayer                  [3, 64, 1, 2]                 \n",
            "  0                -1  1      1792  models.common.vggLayer                  [3, 64, 1, 2]                 \n",
            "INFO:yolov5:  1                -1  1     73856  models.common.vggLayer                  [64, 128, 1, 2]               \n",
            "  1                -1  1     73856  models.common.vggLayer                  [64, 128, 1, 2]               \n",
            "INFO:yolov5:  2                -1  1      8160  models.common.BottleneckCSP             [128, 32, 1]                  \n",
            "  2                -1  1      8160  models.common.BottleneckCSP             [128, 32, 1]                  \n",
            "INFO:yolov5:  3                -1  1     73984  models.common.vggLayer                  [32, 256, 1, 2]               \n",
            "  3                -1  1     73984  models.common.vggLayer                  [32, 256, 1, 2]               \n",
            "INFO:yolov5:  4                -1  3     52928  models.common.BottleneckCSP             [256, 64, 3]                  \n",
            "  4                -1  3     52928  models.common.BottleneckCSP             [256, 64, 3]                  \n",
            "INFO:yolov5:  5                -1  1    295424  models.common.vggLayer                  [64, 512, 1, 2]               \n",
            "  5                -1  1    295424  models.common.vggLayer                  [64, 512, 1, 2]               \n",
            "INFO:yolov5:  6                -1  3    210304  models.common.BottleneckCSP             [512, 128, 3]                 \n",
            "  6                -1  3    210304  models.common.BottleneckCSP             [512, 128, 3]                 \n",
            "INFO:yolov5:  7                -1  1    590336  models.common.vggLayer                  [128, 512, 1, 2]              \n",
            "  7                -1  1    590336  models.common.vggLayer                  [128, 512, 1, 2]              \n",
            "INFO:yolov5:  8                -1  1    394240  models.common.SPP                       [512, 256, [5, 9, 13]]        \n",
            "  8                -1  1    394240  models.common.SPP                       [512, 256, [5, 9, 13]]        \n",
            "INFO:yolov5:  9                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            "  9                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            "INFO:yolov5: 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            "INFO:yolov5: 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            "INFO:yolov5: 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            "INFO:yolov5: 13                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 13                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            "INFO:yolov5: 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "INFO:yolov5: 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            "INFO:yolov5: 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            "INFO:yolov5: 17                -1  1     24000  models.common.BottleneckCSP             [128, 64, 1, False]           \n",
            " 17                -1  1     24000  models.common.BottleneckCSP             [128, 64, 1, False]           \n",
            "INFO:yolov5: 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            "INFO:yolov5: 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            "INFO:yolov5: 20                -1  1     78720  models.common.BottleneckCSP             [128, 128, 1, False]          \n",
            " 20                -1  1     78720  models.common.BottleneckCSP             [128, 128, 1, False]          \n",
            "INFO:yolov5: 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            "INFO:yolov5: 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            "INFO:yolov5: 23                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 23                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            "INFO:yolov5: 24      [17, 20, 23]  1     13530  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            " 24      [17, 20, 23]  1     13530  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "INFO:yolov5:custom_YOLOv5s summary: 287 layers, 2764602 parameters, 2764602 gradients, 29.8 GFLOPs\n",
            "custom_YOLOv5s summary: 287 layers, 2764602 parameters, 2764602 gradients, 29.8 GFLOPs\n",
            "INFO:yolov5:\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer (type)\t\t\t\tOutput Shape\t\t\tParam #\n",
            "==================================================================================\n",
            "model.0.layers.0.weight                 [64, 3, 3, 3]\t\t\t1728\n",
            "model.0.layers.0.bias                   torch.Size([64])\t\t64\n",
            "model.1.layers.0.weight                 [128, 64, 3, 3]\t\t\t73728\n",
            "model.1.layers.0.bias                   torch.Size([128])\t\t128\n",
            "model.2.cv1.conv.weight                 [16, 128, 1, 1]\t\t\t2048\n",
            "model.2.cv1.bn.weight                   torch.Size([16])\t\t16\n",
            "model.2.cv1.bn.bias                     torch.Size([16])\t\t16\n",
            "model.2.cv2.weight                      [16, 128, 1, 1]\t\t\t2048\n",
            "model.2.cv3.weight                      [16, 16, 1, 1]\t\t\t256\n",
            "model.2.cv4.conv.weight                 [32, 32, 1, 1]\t\t\t1024\n",
            "model.2.cv4.bn.weight                   torch.Size([32])\t\t32\n",
            "model.2.cv4.bn.bias                     torch.Size([32])\t\t32\n",
            "model.2.bn.weight                       torch.Size([32])\t\t32\n",
            "model.2.bn.bias                         torch.Size([32])\t\t32\n",
            "model.2.m.0.cv1.conv.weight             [16, 16, 1, 1]\t\t\t256\n",
            "model.2.m.0.cv1.bn.weight               torch.Size([16])\t\t16\n",
            "model.2.m.0.cv1.bn.bias                 torch.Size([16])\t\t16\n",
            "model.2.m.0.cv2.conv.weight             [16, 16, 3, 3]\t\t\t2304\n",
            "model.2.m.0.cv2.bn.weight               torch.Size([16])\t\t16\n",
            "model.2.m.0.cv2.bn.bias                 torch.Size([16])\t\t16\n",
            "model.3.layers.0.weight                 [256, 32, 3, 3]\t\t\t73728\n",
            "model.3.layers.0.bias                   torch.Size([256])\t\t256\n",
            "model.4.cv1.conv.weight                 [32, 256, 1, 1]\t\t\t8192\n",
            "model.4.cv1.bn.weight                   torch.Size([32])\t\t32\n",
            "model.4.cv1.bn.bias                     torch.Size([32])\t\t32\n",
            "model.4.cv2.weight                      [32, 256, 1, 1]\t\t\t8192\n",
            "model.4.cv3.weight                      [32, 32, 1, 1]\t\t\t1024\n",
            "model.4.cv4.conv.weight                 [64, 64, 1, 1]\t\t\t4096\n",
            "model.4.cv4.bn.weight                   torch.Size([64])\t\t64\n",
            "model.4.cv4.bn.bias                     torch.Size([64])\t\t64\n",
            "model.4.bn.weight                       torch.Size([64])\t\t64\n",
            "model.4.bn.bias                         torch.Size([64])\t\t64\n",
            "model.4.m.0.cv1.conv.weight             [32, 32, 1, 1]\t\t\t1024\n",
            "model.4.m.0.cv1.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.0.cv1.bn.bias                 torch.Size([32])\t\t32\n",
            "model.4.m.0.cv2.conv.weight             [32, 32, 3, 3]\t\t\t9216\n",
            "model.4.m.0.cv2.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.0.cv2.bn.bias                 torch.Size([32])\t\t32\n",
            "model.4.m.1.cv1.conv.weight             [32, 32, 1, 1]\t\t\t1024\n",
            "model.4.m.1.cv1.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.1.cv1.bn.bias                 torch.Size([32])\t\t32\n",
            "model.4.m.1.cv2.conv.weight             [32, 32, 3, 3]\t\t\t9216\n",
            "model.4.m.1.cv2.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.1.cv2.bn.bias                 torch.Size([32])\t\t32\n",
            "model.4.m.2.cv1.conv.weight             [32, 32, 1, 1]\t\t\t1024\n",
            "model.4.m.2.cv1.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.2.cv1.bn.bias                 torch.Size([32])\t\t32\n",
            "model.4.m.2.cv2.conv.weight             [32, 32, 3, 3]\t\t\t9216\n",
            "model.4.m.2.cv2.bn.weight               torch.Size([32])\t\t32\n",
            "model.4.m.2.cv2.bn.bias                 torch.Size([32])\t\t32\n",
            "model.5.layers.0.weight                 [512, 64, 3, 3]\t\t\t294912\n",
            "model.5.layers.0.bias                   torch.Size([512])\t\t512\n",
            "model.6.cv1.conv.weight                 [64, 512, 1, 1]\t\t\t32768\n",
            "model.6.cv1.bn.weight                   torch.Size([64])\t\t64\n",
            "model.6.cv1.bn.bias                     torch.Size([64])\t\t64\n",
            "model.6.cv2.weight                      [64, 512, 1, 1]\t\t\t32768\n",
            "model.6.cv3.weight                      [64, 64, 1, 1]\t\t\t4096\n",
            "model.6.cv4.conv.weight                 [128, 128, 1, 1]\t\t\t16384\n",
            "model.6.cv4.bn.weight                   torch.Size([128])\t\t128\n",
            "model.6.cv4.bn.bias                     torch.Size([128])\t\t128\n",
            "model.6.bn.weight                       torch.Size([128])\t\t128\n",
            "model.6.bn.bias                         torch.Size([128])\t\t128\n",
            "model.6.m.0.cv1.conv.weight             [64, 64, 1, 1]\t\t\t4096\n",
            "model.6.m.0.cv1.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.0.cv1.bn.bias                 torch.Size([64])\t\t64\n",
            "model.6.m.0.cv2.conv.weight             [64, 64, 3, 3]\t\t\t36864\n",
            "model.6.m.0.cv2.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.0.cv2.bn.bias                 torch.Size([64])\t\t64\n",
            "model.6.m.1.cv1.conv.weight             [64, 64, 1, 1]\t\t\t4096\n",
            "model.6.m.1.cv1.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.1.cv1.bn.bias                 torch.Size([64])\t\t64\n",
            "model.6.m.1.cv2.conv.weight             [64, 64, 3, 3]\t\t\t36864\n",
            "model.6.m.1.cv2.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.1.cv2.bn.bias                 torch.Size([64])\t\t64\n",
            "model.6.m.2.cv1.conv.weight             [64, 64, 1, 1]\t\t\t4096\n",
            "model.6.m.2.cv1.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.2.cv1.bn.bias                 torch.Size([64])\t\t64\n",
            "model.6.m.2.cv2.conv.weight             [64, 64, 3, 3]\t\t\t36864\n",
            "model.6.m.2.cv2.bn.weight               torch.Size([64])\t\t64\n",
            "model.6.m.2.cv2.bn.bias                 torch.Size([64])\t\t64\n",
            "model.7.layers.0.weight                 [512, 128, 3, 3]\t\t\t589824\n",
            "model.7.layers.0.bias                   torch.Size([512])\t\t512\n",
            "model.8.cv1.conv.weight                 [256, 512, 1, 1]\t\t\t131072\n",
            "model.8.cv1.bn.weight                   torch.Size([256])\t\t256\n",
            "model.8.cv1.bn.bias                     torch.Size([256])\t\t256\n",
            "model.8.cv2.conv.weight                 [256, 1024, 1, 1]\t\t\t262144\n",
            "model.8.cv2.bn.weight                   torch.Size([256])\t\t256\n",
            "model.8.cv2.bn.bias                     torch.Size([256])\t\t256\n",
            "model.9.cv1.conv.weight                 [128, 256, 1, 1]\t\t\t32768\n",
            "model.9.cv1.bn.weight                   torch.Size([128])\t\t128\n",
            "model.9.cv1.bn.bias                     torch.Size([128])\t\t128\n",
            "model.9.cv2.weight                      [128, 256, 1, 1]\t\t\t32768\n",
            "model.9.cv3.weight                      [128, 128, 1, 1]\t\t\t16384\n",
            "model.9.cv4.conv.weight                 [256, 256, 1, 1]\t\t\t65536\n",
            "model.9.cv4.bn.weight                   torch.Size([256])\t\t256\n",
            "model.9.cv4.bn.bias                     torch.Size([256])\t\t256\n",
            "model.9.bn.weight                       torch.Size([256])\t\t256\n",
            "model.9.bn.bias                         torch.Size([256])\t\t256\n",
            "model.9.m.0.cv1.conv.weight             [128, 128, 1, 1]\t\t\t16384\n",
            "model.9.m.0.cv1.bn.weight               torch.Size([128])\t\t128\n",
            "model.9.m.0.cv1.bn.bias                 torch.Size([128])\t\t128\n",
            "model.9.m.0.cv2.conv.weight             [128, 128, 3, 3]\t\t\t147456\n",
            "model.9.m.0.cv2.bn.weight               torch.Size([128])\t\t128\n",
            "model.9.m.0.cv2.bn.bias                 torch.Size([128])\t\t128\n",
            "model.10.conv.weight                    [128, 256, 1, 1]\t\t\t32768\n",
            "model.10.bn.weight                      torch.Size([128])\t\t128\n",
            "model.10.bn.bias                        torch.Size([128])\t\t128\n",
            "model.13.cv1.conv.weight                [64, 256, 1, 1]\t\t\t16384\n",
            "model.13.cv1.bn.weight                  torch.Size([64])\t\t64\n",
            "model.13.cv1.bn.bias                    torch.Size([64])\t\t64\n",
            "model.13.cv2.weight                     [64, 256, 1, 1]\t\t\t16384\n",
            "model.13.cv3.weight                     [64, 64, 1, 1]\t\t\t4096\n",
            "model.13.cv4.conv.weight                [128, 128, 1, 1]\t\t\t16384\n",
            "model.13.cv4.bn.weight                  torch.Size([128])\t\t128\n",
            "model.13.cv4.bn.bias                    torch.Size([128])\t\t128\n",
            "model.13.bn.weight                      torch.Size([128])\t\t128\n",
            "model.13.bn.bias                        torch.Size([128])\t\t128\n",
            "model.13.m.0.cv1.conv.weight            [64, 64, 1, 1]\t\t\t4096\n",
            "model.13.m.0.cv1.bn.weight              torch.Size([64])\t\t64\n",
            "model.13.m.0.cv1.bn.bias                torch.Size([64])\t\t64\n",
            "model.13.m.0.cv2.conv.weight            [64, 64, 3, 3]\t\t\t36864\n",
            "model.13.m.0.cv2.bn.weight              torch.Size([64])\t\t64\n",
            "model.13.m.0.cv2.bn.bias                torch.Size([64])\t\t64\n",
            "model.14.conv.weight                    [64, 128, 1, 1]\t\t\t8192\n",
            "model.14.bn.weight                      torch.Size([64])\t\t64\n",
            "model.14.bn.bias                        torch.Size([64])\t\t64\n",
            "model.17.cv1.conv.weight                [32, 128, 1, 1]\t\t\t4096\n",
            "model.17.cv1.bn.weight                  torch.Size([32])\t\t32\n",
            "model.17.cv1.bn.bias                    torch.Size([32])\t\t32\n",
            "model.17.cv2.weight                     [32, 128, 1, 1]\t\t\t4096\n",
            "model.17.cv3.weight                     [32, 32, 1, 1]\t\t\t1024\n",
            "model.17.cv4.conv.weight                [64, 64, 1, 1]\t\t\t4096\n",
            "model.17.cv4.bn.weight                  torch.Size([64])\t\t64\n",
            "model.17.cv4.bn.bias                    torch.Size([64])\t\t64\n",
            "model.17.bn.weight                      torch.Size([64])\t\t64\n",
            "model.17.bn.bias                        torch.Size([64])\t\t64\n",
            "model.17.m.0.cv1.conv.weight            [32, 32, 1, 1]\t\t\t1024\n",
            "model.17.m.0.cv1.bn.weight              torch.Size([32])\t\t32\n",
            "model.17.m.0.cv1.bn.bias                torch.Size([32])\t\t32\n",
            "model.17.m.0.cv2.conv.weight            [32, 32, 3, 3]\t\t\t9216\n",
            "model.17.m.0.cv2.bn.weight              torch.Size([32])\t\t32\n",
            "model.17.m.0.cv2.bn.bias                torch.Size([32])\t\t32\n",
            "model.18.conv.weight                    [64, 64, 3, 3]\t\t\t36864\n",
            "model.18.bn.weight                      torch.Size([64])\t\t64\n",
            "model.18.bn.bias                        torch.Size([64])\t\t64\n",
            "model.20.cv1.conv.weight                [64, 128, 1, 1]\t\t\t8192\n",
            "model.20.cv1.bn.weight                  torch.Size([64])\t\t64\n",
            "model.20.cv1.bn.bias                    torch.Size([64])\t\t64\n",
            "model.20.cv2.weight                     [64, 128, 1, 1]\t\t\t8192\n",
            "model.20.cv3.weight                     [64, 64, 1, 1]\t\t\t4096\n",
            "model.20.cv4.conv.weight                [128, 128, 1, 1]\t\t\t16384\n",
            "model.20.cv4.bn.weight                  torch.Size([128])\t\t128\n",
            "model.20.cv4.bn.bias                    torch.Size([128])\t\t128\n",
            "model.20.bn.weight                      torch.Size([128])\t\t128\n",
            "model.20.bn.bias                        torch.Size([128])\t\t128\n",
            "model.20.m.0.cv1.conv.weight            [64, 64, 1, 1]\t\t\t4096\n",
            "model.20.m.0.cv1.bn.weight              torch.Size([64])\t\t64\n",
            "model.20.m.0.cv1.bn.bias                torch.Size([64])\t\t64\n",
            "model.20.m.0.cv2.conv.weight            [64, 64, 3, 3]\t\t\t36864\n",
            "model.20.m.0.cv2.bn.weight              torch.Size([64])\t\t64\n",
            "model.20.m.0.cv2.bn.bias                torch.Size([64])\t\t64\n",
            "model.21.conv.weight                    [128, 128, 3, 3]\t\t\t147456\n",
            "model.21.bn.weight                      torch.Size([128])\t\t128\n",
            "model.21.bn.bias                        torch.Size([128])\t\t128\n",
            "model.23.cv1.conv.weight                [128, 256, 1, 1]\t\t\t32768\n",
            "model.23.cv1.bn.weight                  torch.Size([128])\t\t128\n",
            "model.23.cv1.bn.bias                    torch.Size([128])\t\t128\n",
            "model.23.cv2.weight                     [128, 256, 1, 1]\t\t\t32768\n",
            "model.23.cv3.weight                     [128, 128, 1, 1]\t\t\t16384\n",
            "model.23.cv4.conv.weight                [256, 256, 1, 1]\t\t\t65536\n",
            "model.23.cv4.bn.weight                  torch.Size([256])\t\t256\n",
            "model.23.cv4.bn.bias                    torch.Size([256])\t\t256\n",
            "model.23.bn.weight                      torch.Size([256])\t\t256\n",
            "model.23.bn.bias                        torch.Size([256])\t\t256\n",
            "model.23.m.0.cv1.conv.weight            [128, 128, 1, 1]\t\t\t16384\n",
            "model.23.m.0.cv1.bn.weight              torch.Size([128])\t\t128\n",
            "model.23.m.0.cv1.bn.bias                torch.Size([128])\t\t128\n",
            "model.23.m.0.cv2.conv.weight            [128, 128, 3, 3]\t\t\t147456\n",
            "model.23.m.0.cv2.bn.weight              torch.Size([128])\t\t128\n",
            "model.23.m.0.cv2.bn.bias                torch.Size([128])\t\t128\n",
            "model.24.m.0.weight                     [30, 64, 1, 1]\t\t\t1920\n",
            "model.24.m.0.bias                       torch.Size([30])\t\t30\n",
            "model.24.m.1.weight                     [30, 128, 1, 1]\t\t\t3840\n",
            "model.24.m.1.bias                       torch.Size([30])\t\t30\n",
            "model.24.m.2.weight                     [30, 256, 1, 1]\t\t\t7680\n",
            "model.24.m.2.bias                       torch.Size([30])\t\t30\n",
            "==================================================================================\n",
            "Total params: 2764602\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from models.yolo import Model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(cfg='models/custom_yolov5s.yaml').to(device)\n",
        "\n",
        "# Print the model summary with layer names and output shapes\n",
        "print(\"Layer (type)\\t\\t\\t\\tOutput Shape\\t\\t\\tParam #\")\n",
        "print(\"==================================================================================\")\n",
        "total_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        num_params = param.numel()\n",
        "        total_params += num_params\n",
        "        if param.dim() == 1:\n",
        "            print(f\"{name:<40}{param.size()}\\t\\t{num_params}\")\n",
        "        else:\n",
        "            print(f\"{name:<40}{list(param.size())}\\t\\t\\t{num_params}\")\n",
        "\n",
        "print(\"==================================================================================\")\n",
        "print(f\"Total params: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFnF_Nz2iMUf"
      },
      "source": [
        "# Run Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DEWpFfxe34s",
        "outputId": "90a32cb4-4c55-44e3-f0b5-1e23e84948fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public IP: 34.124.225.38\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get('https://ipv4.icanhazip.com')\n",
        "public_ip = response.text.strip()\n",
        "\n",
        "print(\"Public IP:\", public_ip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGvwP66Me82z",
        "outputId": "cecf2700-a879-4960-d4b3-8fbc6e2504c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[..................] | fetchMetadata: sill resolveWithNewModule localtunnel@2.0\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.262s\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.124.225.38:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://thick-ravens-repeat.loca.lt\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f0395122050> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f0395122890> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f0396d6a4d0>\n",
            "Connection(0) Check CandidatePair(('172.28.0.12', 60145) -> ('192.168.43.73', 50065)) State.FROZEN -> State.WAITING\n",
            "Connection(0) Check CandidatePair(('172.28.0.12', 60145) -> ('182.2.145.244', 50065)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(0) Check CandidatePair(('172.28.0.12', 60145) -> ('192.168.43.73', 50065)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Connection(0) Check CandidatePair(('172.28.0.12', 60145) -> ('182.2.145.244', 50065)) State.WAITING -> State.IN_PROGRESS\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Connection(0) Check CandidatePair(('172.28.0.12', 60145) -> ('182.2.145.244', 50065)) State.IN_PROGRESS -> State.SUCCEEDED\n",
            "Connection(0) ICE completed\n",
            "ICE connection state is completed\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video ended\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "2023-06-04 08:49:09.463 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 119, in __getattr__\n",
            "    return self[key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 90, in __getitem__\n",
            "    return get_session_state()[key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/safe_session_state.py\", line 111, in __getitem__\n",
            "    raise KeyError(key)\n",
            "KeyError: '_components_callbacks'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 330, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 253, in main\n",
            "    webcam_streamlit()\n",
            "  File \"/content/yolov5/app.py\", line 199, in webcam_streamlit\n",
            "    webrtc_ctx = webrtc_streamer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit_webrtc/component.py\", line 482, in webrtc_streamer\n",
            "    register_callback(element_key=frontend_key, callback=callback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit_webrtc/components_callbacks.py\", line 64, in register_callback\n",
            "    _state._components_callbacks[element_key] = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 121, in __getattr__\n",
            "    raise AttributeError(_missing_attr_error_message(key))\n",
            "AttributeError: st.session_state has no attribute \"_components_callbacks\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038ff096f0> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038ff09f00> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f039520d720>\n",
            "Connection(1) Check CandidatePair(('172.28.0.12', 50004) -> ('192.168.1.6', 63994)) State.FROZEN -> State.WAITING\n",
            "Connection(1) Check CandidatePair(('172.28.0.12', 50004) -> ('175.158.54.182', 13067)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(1) Check CandidatePair(('172.28.0.12', 50004) -> ('192.168.1.6', 63994)) State.WAITING -> State.IN_PROGRESS\n",
            "Connection(1) Check CandidatePair(('172.28.0.12', 50004) -> ('175.158.54.182', 13067)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=538.483385132 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=538.486542741 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=input/1685869584.34119502092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "image 1/1 /content/yolov5/input/1685869584.34119502092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg: 192x256 2 helmets, 1 no-vest, 2 persons, 1 vest, Done. (0.043s)\n",
            "Speed: 0.3ms pre-process, 43.3ms inference, 1.0ms NMS per image at shape (1, 3, 256, 256)\n",
            "Results saved to \u001b[1moutput\u001b[0m\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=input/1685869607.04640902092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "image 1/1 /content/yolov5/input/1685869607.04640902092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg: 192x256 2 helmets, 1 no-vest, 2 persons, 1 vest, Done. (0.051s)\n",
            "Speed: 0.2ms pre-process, 51.2ms inference, 1.0ms NMS per image at shape (1, 3, 256, 256)\n",
            "Results saved to \u001b[1moutput\u001b[0m\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=input/1685869607.77688202092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "image 1/1 /content/yolov5/input/1685869607.77688202092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg: 192x256 2 helmets, 1 no-vest, 2 persons, 1 vest, Done. (0.051s)\n",
            "Speed: 0.2ms pre-process, 50.8ms inference, 1.0ms NMS per image at shape (1, 3, 256, 256)\n",
            "Results saved to \u001b[1moutput\u001b[0m\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=input/1685869607.77688202092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.0, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 8b92e74 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "image 1/1 /content/yolov5/input/1685869607.77688202092_jpg.rf.6a17bcfa0752eee3e5622a3851faf3bd.jpg: 192x256 413 helmets, 12 no-helmets, 96 no-vests, 106 persons, 32 vests, Done. (0.038s)\n",
            "Speed: 0.2ms pre-process, 37.8ms inference, 5.3ms NMS per image at shape (1, 3, 256, 256)\n",
            "Results saved to \u001b[1moutput\u001b[0m\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038ffd1ea0> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038ffd26b0> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f0395145300>\n",
            "Connection(2) Check CandidatePair(('172.28.0.12', 36121) -> ('192.168.43.73', 57935)) State.FROZEN -> State.WAITING\n",
            "Connection(2) Check CandidatePair(('172.28.0.12', 36121) -> ('182.2.145.244', 57935)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(2) Check CandidatePair(('172.28.0.12', 36121) -> ('192.168.43.73', 57935)) State.WAITING -> State.IN_PROGRESS\n",
            "Connection(2) Check CandidatePair(('172.28.0.12', 36121) -> ('182.2.145.244', 57935)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Connection(2) Check CandidatePair(('172.28.0.12', 36121) -> ('182.2.145.244', 57935)) State.IN_PROGRESS -> State.SUCCEEDED\n",
            "Connection(2) ICE completed\n",
            "ICE connection state is completed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video ended\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038ffbeb00> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038ffbc130> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f038ff2c940>\n",
            "Connection(3) Check CandidatePair(('172.28.0.12', 45417) -> ('192.168.43.73', 53834)) State.FROZEN -> State.WAITING\n",
            "Connection(3) Check CandidatePair(('172.28.0.12', 45417) -> ('182.2.145.244', 53834)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(3) Check CandidatePair(('172.28.0.12', 45417) -> ('192.168.43.73', 53834)) State.WAITING -> State.IN_PROGRESS\n",
            "Connection(3) Check CandidatePair(('172.28.0.12', 45417) -> ('182.2.145.244', 53834)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Connection(3) Check CandidatePair(('172.28.0.12', 45417) -> ('182.2.145.244', 53834)) State.IN_PROGRESS -> State.SUCCEEDED\n",
            "Connection(3) ICE completed\n",
            "ICE connection state is completed\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video ended\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038ffadd50> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038ffae200> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f038ffc2800>\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('192.168.43.73', 61235)) State.FROZEN -> State.WAITING\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('182.2.145.244', 61235)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('192.168.43.73', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('182.2.145.244', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "2023-06-04 09:09:46.978 Session with id 3b2c234d-0d7f-4ab8-aae8-bdb1b15ed4c8 is already connected! Connecting to a new session.\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038dec7c40> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038de98670> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f038ffba050>\n",
            "Connection(5) Check CandidatePair(('172.28.0.12', 42809) -> ('192.168.43.73', 61235)) State.FROZEN -> State.WAITING\n",
            "Connection(5) Check CandidatePair(('172.28.0.12', 42809) -> ('182.2.145.244', 61235)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(5) Check CandidatePair(('172.28.0.12', 42809) -> ('192.168.43.73', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "Connection(5) Check CandidatePair(('172.28.0.12', 42809) -> ('182.2.145.244', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=1667.488732075 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Track video received\n",
            "Set <aiortc.rtcrtpreceiver.RemoteStreamTrack object at 0x7f038df4ba30> as an input video track with video_processor <class 'streamlit_webrtc.process.VideoProcessTrack'>\n",
            "Add a track <streamlit_webrtc.process.VideoProcessTrack object at 0x7f038df14280> of kind video to <aiortc.rtcpeerconnection.RTCPeerConnection object at 0x7f038ffba830>\n",
            "Connection(6) Check CandidatePair(('172.28.0.12', 48940) -> ('192.168.43.73', 61235)) State.FROZEN -> State.WAITING\n",
            "Connection(6) Check CandidatePair(('172.28.0.12', 48940) -> ('182.2.145.244', 61235)) State.FROZEN -> State.WAITING\n",
            "ICE connection state is checking\n",
            "Connection(6) Check CandidatePair(('172.28.0.12', 48940) -> ('192.168.43.73', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=1667.651777055 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "Connection(6) Check CandidatePair(('172.28.0.12', 48940) -> ('182.2.145.244', 61235)) State.WAITING -> State.IN_PROGRESS\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "2023-06-04 09:09:53.564 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 119, in __getattr__\n",
            "    return self[key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 90, in __getitem__\n",
            "    return get_session_state()[key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/safe_session_state.py\", line 111, in __getitem__\n",
            "    raise KeyError(key)\n",
            "KeyError: '_components_callbacks'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/yolov5/app.py\", line 330, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov5/app.py\", line 253, in main\n",
            "    webcam_streamlit()\n",
            "  File \"/content/yolov5/app.py\", line 199, in webcam_streamlit\n",
            "    webrtc_ctx = webrtc_streamer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit_webrtc/component.py\", line 482, in webrtc_streamer\n",
            "    register_callback(element_key=frontend_key, callback=callback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit_webrtc/components_callbacks.py\", line 64, in register_callback\n",
            "    _state._components_callbacks[element_key] = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/state/session_state_proxy.py\", line 121, in __getattr__\n",
            "    raise AttributeError(_missing_attr_error_message(key))\n",
            "AttributeError: st.session_state has no attribute \"_components_callbacks\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "ICE connection state is closed\n",
            "Downloading: \"https://github.com/rickyfazaa/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-6-4 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "custom_YOLOv5s summary: 232 layers, 7257306 parameters, 0 gradients, 16.8 GFLOPs\n",
            "Adding AutoShape... \n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=1699.238814349 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "Exception in callback Transaction.__retry()\n",
            "handle: <TimerHandle when=1699.242158967 Transaction.__retry()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1062, in sendto\n",
            "    self._sock.sendto(data, addr)\n",
            "AttributeError: 'NoneType' object has no attribute 'sendto'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/stun.py\", line 312, in __retry\n",
            "    self.__protocol.send_stun(self.__request, self.__addr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aioice/ice.py\", line 266, in send_stun\n",
            "    self.transport.sendto(bytes(message), addr)\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 1072, in sendto\n",
            "    self._fatal_error(\n",
            "  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 719, in _fatal_error\n",
            "    self._loop.call_exception_handler({\n",
            "AttributeError: 'NoneType' object has no attribute 'call_exception_handler'\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('192.168.43.73', 61235)) State.IN_PROGRESS -> State.FAILED\n",
            "Connection(4) Check CandidatePair(('172.28.0.12', 45553) -> ('182.2.145.244', 61235)) State.IN_PROGRESS -> State.FAILED\n",
            "Connection(4) ICE failed\n",
            "ICE connection state is failed\n",
            "ICE connection state is closed\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFLjP5fJj4nH"
      },
      "source": [
        "# **[Opsional]** Buat ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXvjI5Uqj3QZ",
        "outputId": "7b7fbf71-f16c-4482-f2c6-57c13b156fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/yolov5/ (stored 0%)\n",
            "  adding: content/yolov5/CONTRIBUTING.md (deflated 56%)\n",
            "  adding: content/yolov5/models/ (stored 0%)\n",
            "  adding: content/yolov5/models/__init__.py (stored 0%)\n",
            "  adding: content/yolov5/models/yolov5n.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/experimental.py (deflated 58%)\n",
            "  adding: content/yolov5/models/hub/ (stored 0%)\n",
            "  adding: content/yolov5/models/hub/yolov3-tiny.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/hub/yolov5s6.yaml (deflated 65%)\n",
            "  adding: content/yolov5/models/hub/yolov5-fpn.yaml (deflated 57%)\n",
            "  adding: content/yolov5/models/hub/yolov5-bifpn.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/hub/yolov5-p34.yaml (deflated 62%)\n",
            "  adding: content/yolov5/models/hub/yolov5l6.yaml (deflated 65%)\n",
            "  adding: content/yolov5/models/hub/yolov3-spp.yaml (deflated 63%)\n",
            "  adding: content/yolov5/models/hub/yolov5m6.yaml (deflated 65%)\n",
            "  adding: content/yolov5/models/hub/yolov5-p2.yaml (deflated 66%)\n",
            "  adding: content/yolov5/models/hub/yolov5n6.yaml (deflated 65%)\n",
            "  adding: content/yolov5/models/hub/yolov5x6.yaml (deflated 65%)\n",
            "  adding: content/yolov5/models/hub/yolov5-p7.yaml (deflated 69%)\n",
            "  adding: content/yolov5/models/hub/yolov3.yaml (deflated 63%)\n",
            "  adding: content/yolov5/models/hub/anchors.yaml (deflated 72%)\n",
            "  adding: content/yolov5/models/hub/yolov5s-transformer.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/hub/yolov5s-ghost.yaml (deflated 62%)\n",
            "  adding: content/yolov5/models/hub/yolov5-p6.yaml (deflated 66%)\n",
            "  adding: content/yolov5/models/hub/yolov5-panet.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/yolov5l.yaml (deflated 61%)\n",
            "  adding: content/yolov5/models/common.py (deflated 72%)\n",
            "  adding: content/yolov5/models/tf.py (deflated 73%)\n",
            "  adding: content/yolov5/models/yolov5m.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/yolo.py (deflated 65%)\n",
            "  adding: content/yolov5/models/yolov5s.yaml (deflated 60%)\n",
            "  adding: content/yolov5/models/__pycache__/ (stored 0%)\n",
            "  adding: content/yolov5/models/__pycache__/yolo.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/yolov5/models/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
            "  adding: content/yolov5/models/__pycache__/experimental.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/yolov5/models/__pycache__/common.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/yolov5/models/yolov5x.yaml (deflated 60%)\n",
            "  adding: content/yolov5/hubconf.py (deflated 73%)\n",
            "  adding: content/yolov5/export.py (deflated 73%)\n",
            "  adding: content/yolov5/app.py (deflated 67%)\n",
            "  adding: content/yolov5/data/ (stored 0%)\n",
            "  adding: content/yolov5/data/VisDrone.yaml (deflated 59%)\n",
            "  adding: content/yolov5/data/images/ (stored 0%)\n",
            "  adding: content/yolov5/data/images/bus.jpg (deflated 1%)\n",
            "  adding: content/yolov5/data/images/zidane.jpg (deflated 0%)\n",
            "  adding: content/yolov5/data/coco128.yaml (deflated 50%)\n",
            "  adding: content/yolov5/data/output/ (stored 0%)\n",
            "  adding: content/yolov5/data/output/output.txt (stored 0%)\n",
            "  adding: content/yolov5/data/input/ (stored 0%)\n",
            "  adding: content/yolov5/data/input/input.txt (stored 0%)\n",
            "  adding: content/yolov5/data/hyps/ (stored 0%)\n",
            "  adding: content/yolov5/data/hyps/hyp.scratch-med.yaml (deflated 53%)\n",
            "  adding: content/yolov5/data/hyps/hyp.scratch-low.yaml (deflated 54%)\n",
            "  adding: content/yolov5/data/hyps/hyp.Objects365.yaml (deflated 40%)\n",
            "  adding: content/yolov5/data/hyps/hyp.VOC.yaml (deflated 48%)\n",
            "  adding: content/yolov5/data/hyps/hyp.scratch-high.yaml (deflated 53%)\n",
            "  adding: content/yolov5/data/xView.yaml (deflated 58%)\n",
            "  adding: content/yolov5/data/scripts/ (stored 0%)\n",
            "  adding: content/yolov5/data/scripts/get_coco.sh (deflated 47%)\n",
            "  adding: content/yolov5/data/scripts/download_weights.sh (deflated 37%)\n",
            "  adding: content/yolov5/data/scripts/get_coco128.sh (deflated 36%)\n",
            "  adding: content/yolov5/data/Argoverse.yaml (deflated 56%)\n",
            "  adding: content/yolov5/data/SKU-110K.yaml (deflated 52%)\n",
            "  adding: content/yolov5/data/coco.yaml (deflated 52%)\n",
            "  adding: content/yolov5/data/VOC.yaml (deflated 56%)\n",
            "  adding: content/yolov5/data/GlobalWheat2020.yaml (deflated 54%)\n",
            "  adding: content/yolov5/data/Objects365.yaml (deflated 57%)\n",
            "  adding: content/yolov5/.git/ (stored 0%)\n",
            "  adding: content/yolov5/.git/packed-refs (deflated 11%)\n",
            "  adding: content/yolov5/.git/config (deflated 33%)\n",
            "  adding: content/yolov5/.git/objects/ (stored 0%)\n",
            "  adding: content/yolov5/.git/objects/info/ (stored 0%)\n",
            "  adding: content/yolov5/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/yolov5/.git/objects/pack/pack-d705bf2fe0d28840263f397bd4dcfaf42559382a.idx (deflated 2%)\n",
            "  adding: content/yolov5/.git/objects/pack/pack-d705bf2fe0d28840263f397bd4dcfaf42559382a.pack (deflated 1%)\n",
            "  adding: content/yolov5/.git/info/ (stored 0%)\n",
            "  adding: content/yolov5/.git/info/exclude (deflated 28%)\n",
            "  adding: content/yolov5/.git/logs/ (stored 0%)\n",
            "  adding: content/yolov5/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/yolov5/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/yolov5/.git/logs/refs/heads/master (deflated 27%)\n",
            "  adding: content/yolov5/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/yolov5/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/yolov5/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "  adding: content/yolov5/.git/logs/HEAD (deflated 27%)\n",
            "  adding: content/yolov5/.git/refs/ (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/yolov5/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/yolov5/.git/hooks/ (stored 0%)\n",
            "  adding: content/yolov5/.git/hooks/fsmonitor-watchman.sample (deflated 52%)\n",
            "  adding: content/yolov5/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/yolov5/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/yolov5/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/yolov5/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/yolov5/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/yolov5/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: content/yolov5/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/yolov5/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/yolov5/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/yolov5/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/yolov5/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/yolov5/.git/branches/ (stored 0%)\n",
            "  adding: content/yolov5/.git/description (deflated 14%)\n",
            "  adding: content/yolov5/.git/HEAD (stored 0%)\n",
            "  adding: content/yolov5/.git/index (deflated 54%)\n",
            "  adding: content/yolov5/.gitignore (deflated 55%)\n",
            "  adding: content/yolov5/output/ (stored 0%)\n",
            "  adding: content/yolov5/best.pt (deflated 10%)\n",
            "  adding: content/yolov5/train.py (deflated 69%)\n",
            "  adding: content/yolov5/input/ (stored 0%)\n",
            "  adding: content/yolov5/val.py (deflated 67%)\n",
            "  adding: content/yolov5/.gitattributes (deflated 4%)\n",
            "  adding: content/yolov5/req.txt (deflated 45%)\n",
            "  adding: content/yolov5/.github/ (stored 0%)\n",
            "  adding: content/yolov5/.github/PULL_REQUEST_TEMPLATE.md (deflated 40%)\n",
            "  adding: content/yolov5/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
            "  adding: content/yolov5/.github/ISSUE_TEMPLATE/feature-request.yml (deflated 61%)\n",
            "  adding: content/yolov5/.github/ISSUE_TEMPLATE/question.yml (deflated 57%)\n",
            "  adding: content/yolov5/.github/ISSUE_TEMPLATE/bug-report.yml (deflated 59%)\n",
            "  adding: content/yolov5/.github/ISSUE_TEMPLATE/config.yml (deflated 36%)\n",
            "  adding: content/yolov5/.github/README_cn.md (deflated 69%)\n",
            "  adding: content/yolov5/.github/dependabot.yml (deflated 60%)\n",
            "  adding: content/yolov5/.github/CODE_OF_CONDUCT.md (deflated 60%)\n",
            "  adding: content/yolov5/.github/workflows/ (stored 0%)\n",
            "  adding: content/yolov5/.github/workflows/stale.yml (deflated 59%)\n",
            "  adding: content/yolov5/.github/workflows/rebase.yml (deflated 45%)\n",
            "  adding: content/yolov5/.github/workflows/ci-testing.yml (deflated 69%)\n",
            "  adding: content/yolov5/.github/workflows/codeql-analysis.yml (deflated 52%)\n",
            "  adding: content/yolov5/.github/workflows/docker.yml (deflated 66%)\n",
            "  adding: content/yolov5/.github/workflows/greetings.yml (deflated 62%)\n",
            "  adding: content/yolov5/.github/SECURITY.md (deflated 33%)\n",
            "  adding: content/yolov5/requirements.txt (deflated 52%)\n",
            "  adding: content/yolov5/.pre-commit-config.yaml (deflated 60%)\n",
            "  adding: content/yolov5/setup.cfg (deflated 46%)\n",
            "  adding: content/yolov5/tutorial.ipynb (deflated 79%)\n",
            "  adding: content/yolov5/__pycache__/ (stored 0%)\n",
            "  adding: content/yolov5/__pycache__/video_predict.cpython-310.pyc (deflated 29%)\n",
            "  adding: content/yolov5/__pycache__/export.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/yolov5/LICENSE (deflated 66%)\n",
            "  adding: content/yolov5/README.md (deflated 72%)\n",
            "  adding: content/yolov5/utils/ (stored 0%)\n",
            "  adding: content/yolov5/utils/dataloaders.py (deflated 71%)\n",
            "  adding: content/yolov5/utils/plots.py (deflated 65%)\n",
            "  adding: content/yolov5/utils/activations.py (deflated 68%)\n",
            "  adding: content/yolov5/utils/general.py (deflated 65%)\n",
            "  adding: content/yolov5/utils/callbacks.py (deflated 69%)\n",
            "  adding: content/yolov5/utils/__init__.py (deflated 46%)\n",
            "  adding: content/yolov5/utils/google_app_engine/ (stored 0%)\n",
            "  adding: content/yolov5/utils/google_app_engine/additional_requirements.txt (deflated 10%)\n",
            "  adding: content/yolov5/utils/google_app_engine/app.yaml (deflated 26%)\n",
            "  adding: content/yolov5/utils/google_app_engine/Dockerfile (deflated 47%)\n",
            "  adding: content/yolov5/utils/docker/ (stored 0%)\n",
            "  adding: content/yolov5/utils/docker/.dockerignore (deflated 56%)\n",
            "  adding: content/yolov5/utils/docker/Dockerfile-arm64 (deflated 51%)\n",
            "  adding: content/yolov5/utils/docker/Dockerfile (deflated 54%)\n",
            "  adding: content/yolov5/utils/docker/Dockerfile-cpu (deflated 51%)\n",
            "  adding: content/yolov5/utils/autoanchor.py (deflated 60%)\n",
            "  adding: content/yolov5/utils/torch_utils.py (deflated 63%)\n",
            "  adding: content/yolov5/utils/autobatch.py (deflated 55%)\n",
            "  adding: content/yolov5/utils/loss.py (deflated 69%)\n",
            "  adding: content/yolov5/utils/downloads.py (deflated 65%)\n",
            "  adding: content/yolov5/utils/aws/ (stored 0%)\n",
            "  adding: content/yolov5/utils/aws/__init__.py (stored 0%)\n",
            "  adding: content/yolov5/utils/aws/userdata.sh (deflated 47%)\n",
            "  adding: content/yolov5/utils/aws/resume.py (deflated 46%)\n",
            "  adding: content/yolov5/utils/aws/mime.sh (deflated 46%)\n",
            "  adding: content/yolov5/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/yolov5/utils/__pycache__/dataloaders.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/yolov5/utils/__pycache__/general.cpython-310.pyc (deflated 49%)\n",
            "  adding: content/yolov5/utils/__pycache__/downloads.cpython-310.pyc (deflated 42%)\n",
            "  adding: content/yolov5/utils/__pycache__/autoanchor.cpython-310.pyc (deflated 42%)\n",
            "  adding: content/yolov5/utils/__pycache__/torch_utils.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/yolov5/utils/__pycache__/__init__.cpython-310.pyc (deflated 28%)\n",
            "  adding: content/yolov5/utils/__pycache__/metrics.cpython-310.pyc (deflated 47%)\n",
            "  adding: content/yolov5/utils/__pycache__/plots.cpython-310.pyc (deflated 44%)\n",
            "  adding: content/yolov5/utils/__pycache__/augmentations.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/yolov5/utils/augmentations.py (deflated 63%)\n",
            "  adding: content/yolov5/utils/benchmarks.py (deflated 69%)\n",
            "  adding: content/yolov5/utils/flask_rest_api/ (stored 0%)\n",
            "  adding: content/yolov5/utils/flask_rest_api/restapi.py (deflated 46%)\n",
            "  adding: content/yolov5/utils/flask_rest_api/README.md (deflated 52%)\n",
            "  adding: content/yolov5/utils/flask_rest_api/example_request.py (deflated 26%)\n",
            "  adding: content/yolov5/utils/loggers/ (stored 0%)\n",
            "  adding: content/yolov5/utils/loggers/__init__.py (deflated 66%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/ (stored 0%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/sweep.yaml (deflated 73%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/sweep.py (deflated 53%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/wandb_utils.py (deflated 75%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/__init__.py (stored 0%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/log_dataset.py (deflated 52%)\n",
            "  adding: content/yolov5/utils/loggers/wandb/README.md (deflated 62%)\n",
            "  adding: content/yolov5/utils/metrics.py (deflated 66%)\n",
            "  adding: content/yolov5/detect.py (deflated 68%)\n",
            "  adding: content/yolov5/video_predict.py (deflated 52%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/yolov5.zip /content/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPmh1lENkJ1R",
        "outputId": "7b2ef502-9e86-4dcb-ef93-e364fbdc2f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_64XP-jkS_R"
      },
      "outputs": [],
      "source": [
        "%cp /content/yolov5.zip /content/gdrive/MyDrive/epoch-100-27-mei/arsitektur-1/epoch-300"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "U0NIea35Tkxi",
        "rg1klzQCiI5v",
        "0eg6QtRkASpx",
        "Df5HG5tQBFgF",
        "cFLjP5fJj4nH"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}